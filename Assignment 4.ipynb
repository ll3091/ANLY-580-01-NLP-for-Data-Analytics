{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from https://nlpforhackers.io/topic-modeling/\n",
    "# and applied to new corpus:\n",
    "# https://github.com/andrewts129/transcript-scraping/\n",
    "# tree/master/Donald%20Trump%20Speeches\n",
    "\n",
    "# import packages for assignment\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean text by tokenizing & filtering stop words\n",
    "def clean_text(text):\n",
    "    sw = stopwords.words('english')\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text \n",
    "                    if t not in sw and \n",
    "                    re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    "\n",
    "# function that processes corpus, builds LSI and \n",
    "# LDA models, and output topics for each model\n",
    "def compare_topics(data, n_topics, words):\n",
    "    # clean each document in corpus\n",
    "    tokenized_data = []\n",
    "    for doc in data:\n",
    "        tokenized_data.append(clean_text(doc))\n",
    "\n",
    "    # build a dictionary - association word to numeric id\n",
    "    dictionary = corpora.Dictionary(tokenized_data)\n",
    "\n",
    "    # transform collection of documents to numerical form\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "    # build LSI(LSA) model\n",
    "    lsi_model = models.LsiModel(corpus=corpus, \n",
    "                                num_topics=n_topics, \n",
    "                                id2word=dictionary)\n",
    "\n",
    "    # build LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus, \n",
    "                                num_topics=n_topics, \n",
    "                                id2word=dictionary)\n",
    "\n",
    "    print(\"\\n\\nLSI Model:\")\n",
    "    for i in range(n_topics):\n",
    "        # print the first n most representative topics\n",
    "        print(\"Topic #%s:\" % (i+1), lsi_model.print_topic(i, words))\n",
    "\n",
    "    print(\"\\n\\nLDA Model:\") \n",
    "    for i in range(n_topics):\n",
    "        # print the first n most representative topics\n",
    "        print(\"Topic #%s:\" % (i+1), lda_model.print_topic(i, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 [48255, 4381, 4434, 4676, 5051, 4367, 4352, 4593, 3101, 1910]\n",
      "\n",
      "\n",
      "LSI Model:\n",
      "Topic #1: 0.442*\"going\" + 0.356*\"know\" + 0.293*\"people\" + 0.214*\"said\" + 0.184*\"want\"\n",
      "Topic #2: 0.511*\"going\" + -0.374*\"know\" + -0.312*\"said\" + -0.166*\"like\" + 0.162*\"back\"\n",
      "Topic #3: 0.452*\"going\" + -0.406*\"people\" + -0.246*\"country\" + 0.200*\"know\" + -0.183*\"jobs\"\n",
      "Topic #4: -0.366*\"going\" + 0.263*\"think\" + 0.199*\"want\" + -0.194*\"great\" + -0.190*\"country\"\n",
      "Topic #5: -0.366*\"trump\" + -0.336*\"donald\" + -0.308*\"question\" + -0.257*\"inaudible\" + -0.171*\"think\"\n",
      "Topic #6: 0.264*\"said\" + 0.255*\"wall\" + 0.238*\"want\" + -0.165*\"know\" + -0.164*\"money\"\n",
      "Topic #7: 0.325*\"hillary\" + -0.285*\"people\" + 0.265*\"clinton\" + -0.262*\"going\" + 0.168*\"jobs\"\n",
      "Topic #8: 0.216*\"get\" + 0.213*\"need\" + 0.204*\"israel\" + -0.193*\"people\" + 0.190*\"deal\"\n",
      "Topic #9: 0.349*\"people\" + 0.307*\"thank\" + -0.287*\"jobs\" + -0.230*\"china\" + 0.228*\"want\"\n",
      "Topic #10: -0.399*\"want\" + 0.382*\"people\" + -0.259*\"great\" + -0.243*\"thank\" + 0.178*\"clinton\"\n",
      "\n",
      "\n",
      "LDA Model:\n",
      "Topic #1: 0.039*\"going\" + 0.019*\"people\" + 0.013*\"know\" + 0.011*\"great\" + 0.010*\"want\"\n",
      "Topic #2: 0.019*\"going\" + 0.014*\"people\" + 0.011*\"know\" + 0.011*\"think\" + 0.009*\"right\"\n",
      "Topic #3: 0.038*\"going\" + 0.015*\"people\" + 0.010*\"know\" + 0.009*\"said\" + 0.008*\"great\"\n",
      "Topic #4: 0.023*\"going\" + 0.021*\"people\" + 0.018*\"know\" + 0.010*\"great\" + 0.010*\"country\"\n",
      "Topic #5: 0.017*\"going\" + 0.015*\"people\" + 0.011*\"know\" + 0.009*\"one\" + 0.009*\"country\"\n",
      "Topic #6: 0.016*\"going\" + 0.014*\"people\" + 0.011*\"said\" + 0.011*\"know\" + 0.009*\"right\"\n",
      "Topic #7: 0.019*\"going\" + 0.015*\"people\" + 0.011*\"country\" + 0.010*\"know\" + 0.009*\"great\"\n",
      "Topic #8: 0.024*\"going\" + 0.015*\"know\" + 0.010*\"people\" + 0.009*\"get\" + 0.009*\"said\"\n",
      "Topic #9: 0.020*\"going\" + 0.017*\"people\" + 0.017*\"know\" + 0.015*\"said\" + 0.011*\"great\"\n",
      "Topic #10: 0.025*\"know\" + 0.021*\"going\" + 0.011*\"people\" + 0.009*\"great\" + 0.009*\"get\"\n"
     ]
    }
   ],
   "source": [
    "# read in corpus for modeling\n",
    "with open('AllSpeechesTrump.txt', encoding='utf-8') as inputfile:\n",
    "    data = inputfile.read().split('\\n\\n')\n",
    "\n",
    "# print out number of documents and lengths of first 10 documents\n",
    "print(len(data), [len(d) for d in data][:10])\n",
    "\n",
    "# generate topics\n",
    "compare_topics(data, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models appear to work somewhat similarly on this corpus. Since the corpus is a collection of Trump's campaign speeches, the topics should reflect what he promises to do as president. So it's not surprising that words like - going, people, great, country, want, know, hillary, clinton- appear in both models.\n",
    "\n",
    "From the LSI model, we get other topics like:\n",
    "    wall, money, jobs, china, israel, deal, donald, and trump.\n",
    "\n",
    "From the LDA model, we don't really get any other topics. We get a better variety of topics with the LSI model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
